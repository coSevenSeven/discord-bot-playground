import datetime
import time
from datetime import date, timedelta, timezone
from typing import List, TypedDict

import requests
from bs4 import BeautifulSoup


class Article(TypedDict):
    title: str
    date: str
    url: str
    queryAt: datetime


class PttScraperError(Exception):
    """ç”¨æ–¼æ‰€æœ‰ PTT çˆ¬èŸ²ç›¸é—œéŒ¯èª¤çš„å®¢è£½åŒ–ç•°å¸¸ (ä¾‹å¦‚é€£ç·šå¤±æ•—ã€HTTPéŒ¯èª¤ã€CookieéŒ¯èª¤ç­‰)"""

    pass


base_url = "https://www.ptt.cc"

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "zh-TW,zh;q=0.9",
    "Connection": "close",
}


def get_soup(url: str):
    try:
        r = requests.get(url, headers=headers, timeout=5)  # åŠ ä¸Š timeout
        # å¦‚æœç‹€æ…‹ç¢¼ä¸æ˜¯ 200ï¼Œæœƒæ‹‹å‡º HTTPError
        r.raise_for_status()
        # å³ä½¿ä½¿ç”¨äº† Connection: closeï¼Œä¹Ÿå»ºè­° close response
        r.close()
        soup = BeautifulSoup(r.text, "html.parser")
        return soup

    except requests.exceptions.RequestException as e:
        # æ•æ‰æ‰€æœ‰ requests ç›¸é—œçš„ç¶²è·¯/é€£ç·š/HTTP éŒ¯èª¤
        print(f"è«‹æ±‚éŒ¯èª¤: {e}")
        # çµ±ä¸€æ‹‹å‡ºå®¢è£½åŒ–éŒ¯èª¤ï¼Œè®“ä¸Šå±¤çŸ¥é“é€™æ˜¯çˆ¬èŸ²å•é¡Œ
        raise PttScraperError(f"ç¶²è·¯æˆ–HTTPè«‹æ±‚å¤±æ•—: {e}")

    except requests.exceptions.RequestException as e:
        print(f"è«‹æ±‚éŒ¯èª¤: {e}")
        return None


def get_full_url(route: str):
    return f"{base_url}{route}"


# 2. å®šç¾©æ ¼å¼åŒ–é‚è¼¯
def format_date_custom(date_obj):
    """æ ¼å¼åŒ–å–®å€‹æ—¥æœŸç‚º M/DD"""
    month = date_obj.month  # æœˆä»½ (æ•¸å­—ï¼Œä¸è£œé›¶)
    # ä½¿ç”¨ f-string çš„æ ¼å¼åŒ–åŠŸèƒ½ :02d ç¢ºä¿æ—¥æœŸ (day) æ°¸é è£œé›¶åˆ°å…©ä½æ•¸
    day_formatted = f"{date_obj.day:02d}"

    # çµ„åˆçµæœ
    return f"{month}/{day_formatted}"


def get_today_and_yesterday_dates_custom():
    """
    å–å¾—ä»Šå¤©å’Œæ˜¨å¤©çš„æ—¥æœŸï¼Œä¸¦æ ¼å¼åŒ–ç‚º 'M/DD' å­—ä¸² (æœˆä»½ä¸è£œé›¶ï¼Œæ—¥æœŸè£œé›¶)ã€‚

    å›å‚³:
        tuple: (today_date, yesterday_date)
               ä¾‹å¦‚: ('10/01', '9/30')
    """
    # 1. å–å¾—æ—¥æœŸç‰©ä»¶
    today = date.today()
    yesterday = today - timedelta(days=1)

    # 3. æ‡‰ç”¨æ ¼å¼åŒ–
    today_formatted = format_date_custom(today)
    yesterday_formatted = format_date_custom(yesterday)

    return (today_formatted, yesterday_formatted)


def get_ptt_free_articles():
    current_dt_utc = datetime.datetime.now(timezone.utc)
    today_str, yesterday_str = get_today_and_yesterday_dates_custom()

    print(f"TO {today_str}; YE {yesterday_str}")

    url = get_full_url("/bbs/Steam/index.html")

    article_list: List[Article] = []

    is_continue = True

    while is_continue:
        print(f"çˆ¬å–ç¶²å€ {url}")

        soup = None
        max_retries = 3
        for retry_count in range(max_retries + 1):
            try:
                soup = get_soup(url)
                break
            except PttScraperError as e:
                if retry_count < max_retries:
                    print(
                        f"ğŸš¨ é€£ç·šå¤±æ•—ï¼Œç¬¬ {retry_count + 1} æ¬¡é‡è©¦ (ç­‰å¾… 10 ç§’)... éŒ¯èª¤: {e}"
                    )
                    time.sleep(10)
                else:
                    print(
                        f"âŒ å˜—è©¦ {max_retries + 1} æ¬¡å¾Œä»ç„¡æ³•å–å¾—ç¶²é å…§å®¹ï¼Œçµ‚æ­¢çˆ¬èŸ²ä»»å‹™ã€‚"
                    )
                    raise e

        articles = soup.select(".r-list-container .r-ent")

        if len(articles) == 0:
            print("æ–‡ç« æ•¸é‡ç‚º 0")
            is_continue = False
            break

        print(f"å–å¾— {len(articles)} å€‹æ–‡ç« ï¼Œé–‹å§‹è§£æ")
        tmp_list: List[Article] = []

        for article in articles:
            # print(article.prettify(), end="\r\n")

            # éæ¿¾ç¬¦åˆæ¢ä»¶çš„æ–‡ç« 
            date_element = article.find(class_="date")
            if not date_element:
                print("æ‰¾ä¸åˆ°æ—¥æœŸ html tag")
                continue

            date_str = date_element.text.strip()
            if not date_str:
                print("æ‰¾ä¸åˆ°æ—¥æœŸæ–‡å­—")
                continue

            if date_str != today_str and date_str != yesterday_str:
                # DEBUG ç”¨
                # print(f"éå…©å¤©å…§çš„éŠæˆ²æ–‡ç«  {date_str} {today_str} {yesterday_str}")
                continue

            title_element = article.find(class_="title")
            title_str = "ç„¡æ¨™é¡Œ"
            if title_element:
                title_str = title_element.text.strip()

            # DEBUG ç”¨
            # if "é™å…" not in title_str or "Re:" in title_str:
            #     print(f"[éé™å…éŠæˆ²] {date_str} - {title_str}")
            #     continue

            # select = document.querySelectorAll å›å‚³ list
            # select_one = document.querySelector å›å‚³ä¸€å€‹ node ç¯€é»
            aTag = article.select_one(".title a")
            url = "no-url-available"

            if aTag:
                href = aTag.get("href")
                url = get_full_url(href)

            # DEBUG ç”¨
            # print(f"[éŠæˆ²] {date_str} - {title_str}")

            tmp_list.insert(
                0,
                {
                    "title": title_str,
                    "date": date_str,
                    "url": url,
                    "queryAt": current_dt_utc,
                },
            )

        if len(tmp_list) != 0:
            target_list = [
                item
                for item in tmp_list
                if "é™å…" in item["title"] and "Re:" not in item["title"]
            ]

            # DEBUG ç”¨
            # print([i["title"] for i in target_list])

            article_list.extend(target_list)

            pre_btn_element = soup.select_one(".action-bar .btn.wide:nth-child(2)")
            if pre_btn_element:
                pre_url = pre_btn_element.get("href")
                url = get_full_url(pre_url)
                print(f"ä¸Šä¸€é ç¶²å€: {url}")
        else:
            is_continue = False

    return article_list
